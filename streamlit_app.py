import streamlit as st
import os, json, asyncio
import google.generativeai as genai
from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
import proto  # pipë¡œ ë”°ë¡œ ì„¤ì¹˜ä¸è¦ (google-generativeaiê°€ ëŒì–´ì˜µë‹ˆë‹¤)

# import shutil
# os.makedirs("/root/.streamlit", exist_ok=True)
# shutil.copy("secrets.toml", "/root/.streamlit/secrets.toml")

GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
google_api_key = GOOGLE_API_KEY  # langchainì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ëª…

genai.configure(api_key=GOOGLE_API_KEY)

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-2.5-flash")

# Streamlit App UI

st.set_page_config(page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬")

# Replicate Credentials
with st.sidebar:
    st.title("ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬")

st.title("ğŸ”‘ ì‹ í•œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ")
st.subheader("ë¨¸ë¦¬ì•„í”ˆ ğŸ€ ë§ˆì¼€íŒ… ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ?")

st.write("")

st.write("#ìš°ë¦¬ë™ë„¤ #ìˆ¨ì€ë§›ì§‘ #ì†Œìƒê³µì¸ #ë§ˆì¼€íŒ… #ì „ëµ .. ğŸ¤¤")

st.write("")

# image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
# image_html = f"""
# <div style="display: flex; justify-content: center;">
#     <img src="{image_path}" alt="centered image" width="50%">
# </div>
# """
# st.markdown(image_html, unsafe_allow_html=True)

st.write("")

greeting = "ì–´ë–¤ ê°€ê²Œë¥¼ ì–´ë–»ê²Œ ë„ì™€ì¤„ê¹Œìš”?"

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": greeting}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": greeting}]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

def render_chat_message(role: str, content: str):
    with st.chat_message(role):
        st.markdown(content)

genai.configure(api_key=GOOGLE_API_KEY)
tools = [{
    "function_declarations": [{
        "name": "search_merchant",
        "description": "ê°€ë§¹ì ëª…ì„ ì…ë ¥ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
        "parameters": {
            "type": "object",
            "properties": {"merchant_name": {"type":"string"}},
            "required": ["merchant_name"]
        }
    }]
}]
model = genai.GenerativeModel("gemini-2.5-pro", tools=tools)
chat = model.start_chat()

server_params = StdioServerParameters(
    command="uv",
    args=["run","mcp_server.py"],
    env=None
)   

async def process_user_input(prompt):
    """ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” async í•¨ìˆ˜"""
    # 2) MCP ì„œë²„ ì—°ê²°(í™˜ê²½ì— ë§ê²Œ ëª…ë ¹ ìˆ˜ì •)
    # transport, client = await stdio_client("uv", ["run","mcp_server.py"])
    async with stdio_client(server_params) as (read, write):
        # âœ… 2) ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ClientSessionì„ ë§Œë“¤ê³ 
        async with ClientSession(read, write) as session:
            # âœ… 3) ì„¸ì…˜ì„ initialize í•œë‹¤
            await session.initialize()

            # 3) ìœ ì € í•œ ë§ˆë”” â†’ Gemini ì‘ë‹µì—ì„œ function_call 1ê°œë§Œ ì²˜ë¦¬
            resp = chat.send_message(prompt)  # ì˜ˆì‹œ ì…ë ¥
            fc = None
            if resp.candidates and resp.candidates[0].content.parts:
                for p in resp.candidates[0].content.parts:
                    if getattr(p, "function_call", None):
                        fc = p.function_call
                        break

            if fc:
                # 4) MCP tool ì‹¤í–‰
                name = fc.name
                args: dict = proto.Message.to_dict(fc).get("args", {})
                #args = fc.args if isinstance(fc.args, dict) else json.loads(fc.args)
                mcp_result = await session.call_tool(name, args)

                # 5) ê²°ê³¼ë¥¼ function_responseë¡œ ë˜ëŒë ¤ì£¼ê¸°(í…ìŠ¤íŠ¸ë§Œ ë‹¨ìˆœ ì¶”ì¶œ)
                text_out = ""
                for c in getattr(mcp_result, "content", []) or []:
                    if (isinstance(c, dict) and c.get("type")=="text"):
                        text_out += c.get("text","")
                    elif getattr(c, "type", None) == "text":
                        text_out += getattr(c, "text", "")

                final = chat.send_message([{
                    "function_response": {"name": name, "response": {"ok": True, "text": text_out}}
                }])
                return final.text
            else:
                # í•¨ìˆ˜ í˜¸ì¶œ ì—†ìœ¼ë©´ ì¼ë°˜ ë‹µë³€
                return resp.text

# User-provided prompt
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    render_chat_message("user", prompt)

    with st.spinner("Thinking..."):
        try:
            reply = asyncio.run(process_user_input(prompt))
            st.session_state.messages.append({"role": "assistant", "content": reply})
            render_chat_message("assistant", reply)
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.session_state.messages.append({"role": "assistant", "content": error_msg})
            render_chat_message("assistant", error_msg)
